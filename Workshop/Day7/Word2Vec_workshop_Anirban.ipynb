{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_89yP2qzEui",
    "outputId": "3215cbf0-3a51-412a-82c0-c030405caf44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from keras) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from keras) (1.15.0)\n",
      "Requirement already satisfied: pyyaml in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: h5py in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: keras_applications>=1.0.6 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: keras_preprocessing>=1.0.5 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: tensorflow in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (0.10.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (1.31.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (50.3.0.post20201006)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.22.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: h5py in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.8.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.10.0.0)\n",
      "Requirement already satisfied: plot_keras_history in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (1.1.29)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from plot_keras_history) (1.4.1)\n",
      "Requirement already satisfied: sanitize-ml-labels in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from plot_keras_history) (1.0.26)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from plot_keras_history) (3.4.3)\n",
      "Requirement already satisfied: pandas in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from plot_keras_history) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from scipy->plot_keras_history) (1.19.2)\n",
      "Requirement already satisfied: compress-json in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from sanitize-ml-labels->plot_keras_history) (1.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from matplotlib->plot_keras_history) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from matplotlib->plot_keras_history) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from matplotlib->plot_keras_history) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from matplotlib->plot_keras_history) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from matplotlib->plot_keras_history) (8.3.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from pandas->plot_keras_history) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->plot_keras_history) (1.15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from seaborn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from seaborn) (3.4.3)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from seaborn) (1.1.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from matplotlib>=2.2->seaborn) (8.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: six in c:\\anaconda\\envs\\pyakc\\lib\\site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install plot_keras_history\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AtQk7TU5KqtU",
    "outputId": "54f8a620-a3b1-4dfe-8dfa-dfed517453c5"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QeiZ_Cqvzoqe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\anaconda\\envs\\pyakc\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda, Reshape\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import dot\n",
    "from tensorflow.keras.activations import relu\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NZazoZFH0BZE",
    "outputId": "0cc25bfd-b5e7-43e9-e7ab-1f59572e49ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anirban\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using nltk tokenizer.  \n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Co69rL7jzrh3",
    "outputId": "6cea7630-ba1b-4b7d-cf2e-205588681cf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ',', 1: '.', 2: 'and', 3: 'choose', 4: 'essentially', 5: 'is', 6: 'language', 7: 'never', 8: 'non-random', 9: 'randomly', 10: 'users', 11: 'words', 12: 'a', 13: 'hypothesis', 14: 'null', 15: 'posits', 16: 'randomness', 17: 'statistical', 18: 'testing', 19: 'uses', 20: 'which', 21: 'at', 22: 'be', 23: 'corpora', 24: 'hence', 25: 'in', 26: 'linguistic', 27: 'look', 28: 'phenomena', 29: 'the', 30: 'true', 31: 'we', 32: 'when', 33: 'will', 34: '(', 35: ')', 36: 'able', 37: 'almost', 38: 'always', 39: 'data', 40: 'enough', 41: 'establish', 42: 'it', 43: 'moreover', 44: 'not', 45: 'shall', 46: 'that', 47: 'there', 48: 'to', 49: 'where', 50: 'arbitrary', 51: 'between', 52: 'corpus', 53: 'demonstrably', 54: 'do', 55: 'does', 56: 'fact', 57: 'frequently', 58: 'have', 59: 'inference', 60: 'relation', 61: 'so', 62: 'studies', 63: 'support', 64: 'two', 65: 'are', 66: 'associations', 67: 'evidence', 68: 'experimental', 69: 'frequencies', 70: 'how', 71: 'of', 72: 'present', 73: 'systematically', 74: 'word', 75: 'been', 76: 'has', 77: 'led', 78: 'literature', 79: 'misleading', 80: 'often', 81: 'or', 82: 'results', 83: 'review', 84: 'show', 85: 'unhelpful', 86: 'used'}\n",
      "23\n",
      "and\n",
      "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]\n",
      "87\n",
      "Vocabulary Size: 88\n",
      "Vocabulary Sample: [(0, ','), (1, '.'), (2, 'and'), (3, 'choose'), (4, 'essentially'), (5, 'is'), (6, 'language'), (7, 'never'), (8, 'non-random'), (9, 'randomly')]\n"
     ]
    }
   ],
   "source": [
    "#Data Preparation \n",
    "\n",
    "AlotOftext = \"\"\"Language users never choose words randomly, and language is essentially\n",
    "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
    "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
    "the null hypothesis will never be true. Moreover, where there is enough\n",
    "data, we shall (almost) always be able to establish that it is not true. In\n",
    "corpus studies, we frequently do have enough data, so the fact that a relation \n",
    "between two phenomena is demonstrably non-random, does not support the inference \n",
    "that it is not arbitrary. We present experimental evidence\n",
    "of how arbitrary associations between word frequencies and corpora are\n",
    "systematically non-random. We review literature in which hypothesis testing \n",
    "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
    "\n",
    "\n",
    "\n",
    "#Tokenize text\n",
    "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(AlotOftext)]\n",
    "tokenized_text\n",
    "\n",
    "#Create Vocab as a Dictionary\n",
    "vocab = Dictionary(tokenized_text)\n",
    "print(dict(vocab.items()))\n",
    "\n",
    "print(vocab.token2id['corpora'])\n",
    "print(vocab[2])\n",
    "sent0 = tokenized_text[0]\n",
    "print(vocab.doc2idx(sent0))\n",
    "\n",
    "vocab.add_documents([['PAD']])\n",
    "dict(vocab.items())\n",
    "print(vocab.token2id['PAD'])\n",
    "\n",
    "corpusByWordID = list()\n",
    "for sent in  tokenized_text:\n",
    "    corpusByWordID.append(vocab.doc2idx(sent))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 100\n",
    "hidden_dim=100\n",
    "window_size = 2 # context window size\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBxqCqAL1BCo",
    "outputId": "3e363bc2-9eba-4158-f07a-9e15e1a21ac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['PAD', 'PAD', 'users', 'never'] -> Target (Y): language\n",
      "Context (X): ['PAD', 'language', 'never', 'choose'] -> Target (Y): users\n",
      "Context (X): ['language', 'users', 'choose', 'words'] -> Target (Y): never\n",
      "Context (X): ['users', 'never', 'words', 'randomly'] -> Target (Y): choose\n",
      "Context (X): ['never', 'choose', 'randomly', ','] -> Target (Y): words\n",
      "Context (X): ['choose', 'words', ',', 'and'] -> Target (Y): randomly\n",
      "Context (X): ['words', 'randomly', 'and', 'language'] -> Target (Y): ,\n",
      "Context (X): ['randomly', ',', 'language', 'is'] -> Target (Y): and\n",
      "Context (X): [',', 'and', 'is', 'essentially'] -> Target (Y): language\n",
      "Context (X): ['and', 'language', 'essentially', 'non-random'] -> Target (Y): is\n",
      "Context (X): ['language', 'is', 'non-random', '.'] -> Target (Y): essentially\n",
      "Context (X): ['is', 'essentially', '.', 'PAD'] -> Target (Y): non-random\n",
      "Context (X): ['essentially', 'non-random', 'PAD', 'PAD'] -> Target (Y): .\n",
      "Context (X): ['PAD', 'PAD', 'hypothesis', 'testing'] -> Target (Y): statistical\n",
      "Context (X): ['PAD', 'statistical', 'testing', 'uses'] -> Target (Y): hypothesis\n",
      "Context (X): ['statistical', 'hypothesis', 'uses', 'a'] -> Target (Y): testing\n",
      "Context (X): ['hypothesis', 'testing', 'a', 'null'] -> Target (Y): uses\n",
      "Context (X): ['testing', 'uses', 'null', 'hypothesis'] -> Target (Y): a\n",
      "Context (X): ['uses', 'a', 'hypothesis', ','] -> Target (Y): null\n",
      "Context (X): ['a', 'null', ',', 'which'] -> Target (Y): hypothesis\n",
      "Context (X): ['null', 'hypothesis', 'which', 'posits'] -> Target (Y): ,\n",
      "Context (X): ['hypothesis', ',', 'posits', 'randomness'] -> Target (Y): which\n",
      "Context (X): [',', 'which', 'randomness', '.'] -> Target (Y): posits\n",
      "Context (X): ['which', 'posits', '.', 'PAD'] -> Target (Y): randomness\n",
      "Context (X): ['posits', 'randomness', 'PAD', 'PAD'] -> Target (Y): .\n",
      "Context (X): ['PAD', 'PAD', ',', 'when'] -> Target (Y): hence\n",
      "Context (X): ['PAD', 'hence', 'when', 'we'] -> Target (Y): ,\n",
      "Context (X): ['hence', ',', 'we', 'look'] -> Target (Y): when\n",
      "Context (X): [',', 'when', 'look', 'at'] -> Target (Y): we\n",
      "Context (X): ['when', 'we', 'at', 'linguistic'] -> Target (Y): look\n",
      "Context (X): ['we', 'look', 'linguistic', 'phenomena'] -> Target (Y): at\n",
      "Context (X): ['look', 'at', 'phenomena', 'in'] -> Target (Y): linguistic\n",
      "Context (X): ['at', 'linguistic', 'in', 'corpora'] -> Target (Y): phenomena\n",
      "Context (X): ['linguistic', 'phenomena', 'corpora', ','] -> Target (Y): in\n",
      "Context (X): ['phenomena', 'in', ',', 'the'] -> Target (Y): corpora\n",
      "Context (X): ['in', 'corpora', 'the', 'null'] -> Target (Y): ,\n",
      "Context (X): ['corpora', ',', 'null', 'hypothesis'] -> Target (Y): the\n",
      "Context (X): [',', 'the', 'hypothesis', 'will'] -> Target (Y): null\n",
      "Context (X): ['the', 'null', 'will', 'never'] -> Target (Y): hypothesis\n",
      "Context (X): ['null', 'hypothesis', 'never', 'be'] -> Target (Y): will\n",
      "Context (X): ['hypothesis', 'will', 'be', 'true'] -> Target (Y): never\n",
      "Context (X): ['will', 'never', 'true', '.'] -> Target (Y): be\n",
      "Context (X): ['never', 'be', '.', 'PAD'] -> Target (Y): true\n",
      "Context (X): ['be', 'true', 'PAD', 'PAD'] -> Target (Y): .\n",
      "Context (X): ['PAD', 'PAD', ',', 'where'] -> Target (Y): moreover\n",
      "Context (X): ['PAD', 'moreover', 'where', 'there'] -> Target (Y): ,\n",
      "Context (X): ['moreover', ',', 'there', 'is'] -> Target (Y): where\n",
      "Context (X): [',', 'where', 'is', 'enough'] -> Target (Y): there\n",
      "Context (X): ['where', 'there', 'enough', 'data'] -> Target (Y): is\n",
      "Context (X): ['there', 'is', 'data', ','] -> Target (Y): enough\n",
      "Context (X): ['is', 'enough', ',', 'we'] -> Target (Y): data\n",
      "Context (X): ['enough', 'data', 'we', 'shall'] -> Target (Y): ,\n",
      "Context (X): ['data', ',', 'shall', '('] -> Target (Y): we\n",
      "Context (X): [',', 'we', '(', 'almost'] -> Target (Y): shall\n",
      "Context (X): ['we', 'shall', 'almost', ')'] -> Target (Y): (\n",
      "Context (X): ['shall', '(', ')', 'always'] -> Target (Y): almost\n",
      "Context (X): ['(', 'almost', 'always', 'be'] -> Target (Y): )\n",
      "Context (X): ['almost', ')', 'be', 'able'] -> Target (Y): always\n",
      "Context (X): [')', 'always', 'able', 'to'] -> Target (Y): be\n",
      "Context (X): ['always', 'be', 'to', 'establish'] -> Target (Y): able\n",
      "Context (X): ['be', 'able', 'establish', 'that'] -> Target (Y): to\n",
      "Context (X): ['able', 'to', 'that', 'it'] -> Target (Y): establish\n",
      "Context (X): ['to', 'establish', 'it', 'is'] -> Target (Y): that\n",
      "Context (X): ['establish', 'that', 'is', 'not'] -> Target (Y): it\n",
      "Context (X): ['that', 'it', 'not', 'true'] -> Target (Y): is\n",
      "Context (X): ['it', 'is', 'true', '.'] -> Target (Y): not\n",
      "Context (X): ['is', 'not', '.', 'PAD'] -> Target (Y): true\n",
      "Context (X): ['not', 'true', 'PAD', 'PAD'] -> Target (Y): .\n",
      "Context (X): ['PAD', 'PAD', 'corpus', 'studies'] -> Target (Y): in\n",
      "Context (X): ['PAD', 'in', 'studies', ','] -> Target (Y): corpus\n",
      "Context (X): ['in', 'corpus', ',', 'we'] -> Target (Y): studies\n",
      "Context (X): ['corpus', 'studies', 'we', 'frequently'] -> Target (Y): ,\n",
      "Context (X): ['studies', ',', 'frequently', 'do'] -> Target (Y): we\n",
      "Context (X): [',', 'we', 'do', 'have'] -> Target (Y): frequently\n",
      "Context (X): ['we', 'frequently', 'have', 'enough'] -> Target (Y): do\n",
      "Context (X): ['frequently', 'do', 'enough', 'data'] -> Target (Y): have\n",
      "Context (X): ['do', 'have', 'data', ','] -> Target (Y): enough\n",
      "Context (X): ['have', 'enough', ',', 'so'] -> Target (Y): data\n",
      "Context (X): ['enough', 'data', 'so', 'the'] -> Target (Y): ,\n",
      "Context (X): ['data', ',', 'the', 'fact'] -> Target (Y): so\n",
      "Context (X): [',', 'so', 'fact', 'that'] -> Target (Y): the\n",
      "Context (X): ['so', 'the', 'that', 'a'] -> Target (Y): fact\n",
      "Context (X): ['the', 'fact', 'a', 'relation'] -> Target (Y): that\n",
      "Context (X): ['fact', 'that', 'relation', 'between'] -> Target (Y): a\n",
      "Context (X): ['that', 'a', 'between', 'two'] -> Target (Y): relation\n",
      "Context (X): ['a', 'relation', 'two', 'phenomena'] -> Target (Y): between\n",
      "Context (X): ['relation', 'between', 'phenomena', 'is'] -> Target (Y): two\n",
      "Context (X): ['between', 'two', 'is', 'demonstrably'] -> Target (Y): phenomena\n",
      "Context (X): ['two', 'phenomena', 'demonstrably', 'non-random'] -> Target (Y): is\n",
      "Context (X): ['phenomena', 'is', 'non-random', ','] -> Target (Y): demonstrably\n",
      "Context (X): ['is', 'demonstrably', ',', 'does'] -> Target (Y): non-random\n",
      "Context (X): ['demonstrably', 'non-random', 'does', 'not'] -> Target (Y): ,\n",
      "Context (X): ['non-random', ',', 'not', 'support'] -> Target (Y): does\n",
      "Context (X): [',', 'does', 'support', 'the'] -> Target (Y): not\n",
      "Context (X): ['does', 'not', 'the', 'inference'] -> Target (Y): support\n",
      "Context (X): ['not', 'support', 'inference', 'that'] -> Target (Y): the\n",
      "Context (X): ['support', 'the', 'that', 'it'] -> Target (Y): inference\n",
      "Context (X): ['the', 'inference', 'it', 'is'] -> Target (Y): that\n",
      "Context (X): ['inference', 'that', 'is', 'not'] -> Target (Y): it\n",
      "Context (X): ['that', 'it', 'not', 'arbitrary'] -> Target (Y): is\n",
      "Context (X): ['it', 'is', 'arbitrary', '.'] -> Target (Y): not\n",
      "Context (X): ['is', 'not', '.', 'PAD'] -> Target (Y): arbitrary\n",
      "Context (X): ['not', 'arbitrary', 'PAD', 'PAD'] -> Target (Y): .\n",
      "Context (X): ['PAD', 'PAD', 'present', 'experimental'] -> Target (Y): we\n",
      "Context (X): ['PAD', 'we', 'experimental', 'evidence'] -> Target (Y): present\n",
      "Context (X): ['we', 'present', 'evidence', 'of'] -> Target (Y): experimental\n",
      "Context (X): ['present', 'experimental', 'of', 'how'] -> Target (Y): evidence\n",
      "Context (X): ['experimental', 'evidence', 'how', 'arbitrary'] -> Target (Y): of\n",
      "Context (X): ['evidence', 'of', 'arbitrary', 'associations'] -> Target (Y): how\n",
      "Context (X): ['of', 'how', 'associations', 'between'] -> Target (Y): arbitrary\n",
      "Context (X): ['how', 'arbitrary', 'between', 'word'] -> Target (Y): associations\n",
      "Context (X): ['arbitrary', 'associations', 'word', 'frequencies'] -> Target (Y): between\n",
      "Context (X): ['associations', 'between', 'frequencies', 'and'] -> Target (Y): word\n",
      "Context (X): ['between', 'word', 'and', 'corpora'] -> Target (Y): frequencies\n",
      "Context (X): ['word', 'frequencies', 'corpora', 'are'] -> Target (Y): and\n",
      "Context (X): ['frequencies', 'and', 'are', 'systematically'] -> Target (Y): corpora\n",
      "Context (X): ['and', 'corpora', 'systematically', 'non-random'] -> Target (Y): are\n",
      "Context (X): ['corpora', 'are', 'non-random', '.'] -> Target (Y): systematically\n",
      "Context (X): ['are', 'systematically', '.', 'PAD'] -> Target (Y): non-random\n",
      "Context (X): ['systematically', 'non-random', 'PAD', 'PAD'] -> Target (Y): .\n",
      "Context (X): ['PAD', 'PAD', 'review', 'literature'] -> Target (Y): we\n",
      "Context (X): ['PAD', 'we', 'literature', 'in'] -> Target (Y): review\n",
      "Context (X): ['we', 'review', 'in', 'which'] -> Target (Y): literature\n",
      "Context (X): ['review', 'literature', 'which', 'hypothesis'] -> Target (Y): in\n",
      "Context (X): ['literature', 'in', 'hypothesis', 'testing'] -> Target (Y): which\n",
      "Context (X): ['in', 'which', 'testing', 'has'] -> Target (Y): hypothesis\n",
      "Context (X): ['which', 'hypothesis', 'has', 'been'] -> Target (Y): testing\n",
      "Context (X): ['hypothesis', 'testing', 'been', 'used'] -> Target (Y): has\n",
      "Context (X): ['testing', 'has', 'used', ','] -> Target (Y): been\n",
      "Context (X): ['has', 'been', ',', 'and'] -> Target (Y): used\n",
      "Context (X): ['been', 'used', 'and', 'show'] -> Target (Y): ,\n",
      "Context (X): ['used', ',', 'show', 'how'] -> Target (Y): and\n",
      "Context (X): [',', 'and', 'how', 'it'] -> Target (Y): show\n",
      "Context (X): ['and', 'show', 'it', 'has'] -> Target (Y): how\n",
      "Context (X): ['show', 'how', 'has', 'often'] -> Target (Y): it\n",
      "Context (X): ['how', 'it', 'often', 'led'] -> Target (Y): has\n",
      "Context (X): ['it', 'has', 'led', 'to'] -> Target (Y): often\n",
      "Context (X): ['has', 'often', 'to', 'unhelpful'] -> Target (Y): led\n",
      "Context (X): ['often', 'led', 'unhelpful', 'or'] -> Target (Y): to\n",
      "Context (X): ['led', 'to', 'or', 'misleading'] -> Target (Y): unhelpful\n",
      "Context (X): ['to', 'unhelpful', 'misleading', 'results'] -> Target (Y): or\n",
      "Context (X): ['unhelpful', 'or', 'results', '.'] -> Target (Y): misleading\n",
      "Context (X): ['or', 'misleading', '.', 'PAD'] -> Target (Y): results\n",
      "Context (X): ['misleading', 'results', 'PAD', 'PAD'] -> Target (Y): .\n"
     ]
    }
   ],
   "source": [
    "# Create CBOW Training data\n",
    "def generate_cbow_context_word_pairs(corpusByID, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for sent in corpusByID:\n",
    "        sentence_length = len(sent)\n",
    "        for index, word in enumerate(sent):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([sent[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "            if start<0:\n",
    "                x = sequence.pad_sequences(context_words, maxlen=context_length,padding='pre',value=vocab.token2id['PAD'])\n",
    "                y = np_utils.to_categorical(label_word, vocab_size)\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "                continue\n",
    "            if end>=sentence_length:\n",
    "                x = sequence.pad_sequences(context_words, maxlen=context_length,padding='post',value=vocab.token2id['PAD'])\n",
    "                y = np_utils.to_categorical(label_word, vocab_size)\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "                continue\n",
    "            else:\n",
    "                X.append(sequence.pad_sequences(context_words, maxlen=context_length))\n",
    "                y = np_utils.to_categorical(label_word, vocab_size)\n",
    "                Y.append(y)\n",
    "                continue\n",
    "           \n",
    "    return X,Y\n",
    "            \n",
    "# Test this out for some samples\n",
    "\n",
    "\n",
    "X,Y = generate_cbow_context_word_pairs(corpusByWordID, window_size, vocab_size) \n",
    "   \n",
    "for x, y in zip(X,Y):\n",
    "    print('Context (X):', [vocab[w] for w in x[0]], '-> Target (Y):', vocab[np.argwhere(y[0])[0][0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0_VNvJw3Vd_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JH_iy4Lh1JUa",
    "outputId": "403a35cd-6b77-47b7-b93b-cc3d667ce100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            8800      \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 88)                8888      \n",
      "=================================================================\n",
      "Total params: 17,688\n",
      "Trainable params: 17,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cbow = Sequential()\n",
    "\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=4))\n",
    "cbow.add(Lambda(lambda x: relu(K.mean(x, axis=1)), output_shape=(embed_size,)))\n",
    "\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
    "cbow.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuVqi7WGDxNe",
    "outputId": "37718cd2-70a5-4486-ec65-dddbf6ec86f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\pyakc\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 644.7470927238464\n",
      "1 642.842649936676\n",
      "2 640.999680519104\n",
      "3 639.2180123329163\n",
      "4 637.4932975769043\n",
      "5 635.8243727684021\n",
      "6 634.2167117595673\n",
      "7 632.6695871353149\n",
      "8 631.1827216148376\n",
      "9 629.753434419632\n",
      "10 628.3805961608887\n",
      "11 627.0611128807068\n",
      "12 625.7937846183777\n",
      "13 624.5779800415039\n",
      "14 623.4111428260803\n",
      "15 622.2898659706116\n",
      "16 621.2124836444855\n",
      "17 620.175265789032\n",
      "18 619.1758034229279\n",
      "19 618.2108869552612\n",
      "20 617.2788717746735\n",
      "21 616.3763751983643\n",
      "22 615.50204205513\n",
      "23 614.6517877578735\n",
      "24 613.8225095272064\n",
      "25 613.0132806301117\n",
      "26 612.218649148941\n",
      "27 611.4385242462158\n",
      "28 610.6693015098572\n",
      "29 609.9095468521118\n",
      "30 609.1575658321381\n",
      "31 608.4104459285736\n",
      "32 607.6659007072449\n",
      "33 606.9219174385071\n",
      "34 606.1774842739105\n",
      "35 605.4288029670715\n",
      "36 604.6764075756073\n",
      "37 603.9170689582825\n",
      "38 603.1508069038391\n",
      "39 602.3760697841644\n",
      "40 601.5933713912964\n",
      "41 600.8021206855774\n",
      "42 600.0021638870239\n",
      "43 599.1928384304047\n",
      "44 598.3760108947754\n",
      "45 597.5514776706696\n",
      "46 596.7200784683228\n",
      "47 595.8839212656021\n",
      "48 595.0431594848633\n",
      "49 594.1983315944672\n",
      "50 593.3540380001068\n",
      "51 592.5095723867416\n",
      "52 591.6670348644257\n",
      "53 590.8274558782578\n",
      "54 589.9936397075653\n",
      "55 589.1659290790558\n",
      "56 588.3444837331772\n",
      "57 587.5302041769028\n",
      "58 586.7247292995453\n",
      "59 585.9272003173828\n",
      "60 585.1388636827469\n",
      "61 584.359037399292\n",
      "62 583.5866239070892\n",
      "63 582.823713183403\n",
      "64 582.066930770874\n",
      "65 581.31743735075\n",
      "66 580.5729495882988\n",
      "67 579.8340817689896\n",
      "68 579.0981437563896\n",
      "69 578.3666177392006\n",
      "70 577.637634575367\n",
      "71 576.9102335572243\n",
      "72 576.1858824491501\n",
      "73 575.4625835418701\n",
      "74 574.7378598451614\n",
      "75 574.0158855319023\n",
      "76 573.2906528115273\n",
      "77 572.5653388500214\n",
      "78 571.8371293544769\n",
      "79 571.1071731448174\n",
      "80 570.3755342364311\n",
      "81 569.6396041512489\n",
      "82 568.9026944637299\n",
      "83 568.1613609790802\n",
      "84 567.4172827005386\n",
      "85 566.6690572500229\n",
      "86 565.9157054424286\n",
      "87 565.157582461834\n",
      "88 564.39463108778\n",
      "89 563.6272918581963\n",
      "90 562.8534595966339\n",
      "91 562.0736427307129\n",
      "92 561.2891818284988\n",
      "93 560.4982310533524\n",
      "94 559.7000340223312\n",
      "95 558.8972501754761\n",
      "96 558.0860273241997\n",
      "97 557.267410993576\n",
      "98 556.4416624307632\n",
      "99 555.6097244620323\n",
      "100 554.7688513994217\n",
      "101 553.9214478135109\n",
      "102 553.0638167262077\n",
      "103 552.2011969685555\n",
      "104 551.3278639316559\n",
      "105 550.4486103057861\n",
      "106 549.5594255924225\n",
      "107 548.6596640944481\n",
      "108 547.7540745139122\n",
      "109 546.836788892746\n",
      "110 545.9098573923111\n",
      "111 544.9755036234856\n",
      "112 544.0292982459068\n",
      "113 543.0736857056618\n",
      "114 542.1070747375488\n",
      "115 541.1318426132202\n",
      "116 540.1464514136314\n",
      "117 539.1491207480431\n",
      "118 538.1413570642471\n",
      "119 537.1251601576805\n",
      "120 536.0949249267578\n",
      "121 535.0569598674774\n",
      "122 534.0065371990204\n",
      "123 532.9431838393211\n",
      "124 531.8703548312187\n",
      "125 530.7863680720329\n",
      "126 529.689630150795\n",
      "127 528.5826845169067\n",
      "128 527.4614347815514\n",
      "129 526.3317518830299\n",
      "130 525.188000023365\n",
      "131 524.031379699707\n",
      "132 522.8641493916512\n",
      "133 521.6827608346939\n",
      "134 520.4902442097664\n",
      "135 519.2845879793167\n",
      "136 518.0662509799004\n",
      "137 516.8352265954018\n",
      "138 515.5911014676094\n",
      "139 514.332099199295\n",
      "140 513.0643631219864\n",
      "141 511.78193512558937\n",
      "142 510.48722183704376\n",
      "143 509.1792893111706\n",
      "144 507.8581822514534\n",
      "145 506.5255327820778\n",
      "146 505.17671740055084\n",
      "147 503.8174432516098\n",
      "148 502.4454517364502\n",
      "149 501.059602022171\n",
      "150 499.6606225967407\n",
      "151 498.25002694129944\n",
      "152 496.8250576853752\n",
      "153 495.3902885019779\n",
      "154 493.93839251995087\n",
      "155 492.4770442247391\n",
      "156 491.0041928291321\n",
      "157 489.517220556736\n",
      "158 488.0183277130127\n",
      "159 486.50667810440063\n",
      "160 484.9838040471077\n",
      "161 483.4503465592861\n",
      "162 481.90047934651375\n",
      "163 480.3407485485077\n",
      "164 478.76888939738274\n",
      "165 477.18659114837646\n",
      "166 475.5935456454754\n",
      "167 473.9857165515423\n",
      "168 472.372781008482\n",
      "169 470.74361473321915\n",
      "170 469.1054727137089\n",
      "171 467.4610444307327\n",
      "172 465.80155235528946\n",
      "173 464.13548612594604\n",
      "174 462.45498183369637\n",
      "175 460.76891502738\n",
      "176 459.0697599351406\n",
      "177 457.364116281271\n",
      "178 455.646812915802\n",
      "179 453.9225149154663\n",
      "180 452.18750962615013\n",
      "181 450.44285225868225\n",
      "182 448.69179886579514\n",
      "183 446.9310645163059\n",
      "184 445.16244503855705\n",
      "185 443.38449078798294\n",
      "186 441.59943836927414\n",
      "187 439.8056786060333\n",
      "188 438.00536757707596\n",
      "189 436.194269746542\n",
      "190 434.3823753595352\n",
      "191 432.55341082811356\n",
      "192 430.72462394833565\n",
      "193 428.88444834947586\n",
      "194 427.03851741552353\n",
      "195 425.1849555373192\n",
      "196 423.3236593604088\n",
      "197 421.4601217806339\n",
      "198 419.58059898018837\n",
      "199 417.7037816941738\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = 0.\n",
    "    for x, y in zip(X,Y):\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dxPVT_G-1RYz"
   },
   "outputs": [],
   "source": [
    "## Save the wordvectors\n",
    "f = open('Cbow_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = cbow.get_weights()[0]\n",
    "for key in vocab:\n",
    "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
    "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1H3zTpE1Uwr",
    "outputId": "6040fcb5-d7e3-429d-f2bc-b699215b5339"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('non-random', 0.579158365726471),\n",
       " ('randomly', 0.42462313175201416),\n",
       " ('show', 0.4117108881473541),\n",
       " ('demonstrably', 0.34696492552757263),\n",
       " ('used', 0.34234124422073364),\n",
       " ('phenomena', 0.3087191879749298),\n",
       " ('not', 0.2967388927936554),\n",
       " ('true', 0.2952246069908142),\n",
       " ('there', 0.2752431035041809),\n",
       " ('so', 0.2749215066432953)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the vectors back and validate\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./Cbow_vectors.txt', binary=False)\n",
    "\n",
    "w2v.most_similar(positive=['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4z88oUR1Yk5",
    "outputId": "fd48c34c-720d-47b3-d717-c3dcf66ca6a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(words (11), systematically (73)) -> 0\n",
      "(is (5), non-random (8)) -> 1\n",
      "(never (7), uses (19)) -> 0\n",
      "(choose (3), users (10)) -> 1\n",
      "(randomly (9), able (36)) -> 0\n",
      "(. (1), essentially (4)) -> 1\n",
      "(users (10), choose (3)) -> 1\n",
      "(users (10), language (6)) -> 1\n",
      "(users (10), linguistic (26)) -> 0\n",
      "(never (7), language (6)) -> 1\n"
     ]
    }
   ],
   "source": [
    "#Create Skipgram Training data \n",
    "\n",
    "# generate skip-grams with both positive and negative examples\n",
    "skip_grams = [skipgrams(sent, vocabulary_size=vocab_size, window_size=2) for sent in corpusByWordID]\n",
    "\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "        vocab[pairs[i][0]], pairs[i][0],           \n",
    "        vocab[pairs[i][1]], pairs[i][1], \n",
    "        labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EqD40Iq11fpg",
    "outputId": "6ca49783-58ea-4604-bca0-0a18f691d668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 1, 100)       8800        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conotext_embedding (Embedding)  (None, 1, 100)       8800        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 100, 1)       0           word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 100, 1)       0           conotext_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 1)         0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1)            0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            2           reshape_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,602\n",
      "Trainable params: 17,602\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#define the skip-gram model\n",
    "\n",
    "input_word = Input((1,))\n",
    "input_context_word = Input((1,))\n",
    "\n",
    "word_embedding    = Embedding(input_dim=vocab_size, output_dim=embed_size,input_length=1,name='word_embedding')\n",
    "context_embedding = Embedding(input_dim=vocab_size, output_dim=embed_size,input_length=1,name='conotext_embedding')\n",
    "\n",
    "word_embedding = word_embedding(input_word)\n",
    "word_embedding_layer = Reshape((-1, 1))(word_embedding)\n",
    "\n",
    "context_embedding = context_embedding(input_context_word)\n",
    "context_embedding_layer = Reshape((-1, 1))(context_embedding)\n",
    "\n",
    "# now perform the dot product operation  \n",
    "dot_product = dot([word_embedding_layer, context_embedding_layer], axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "# add the sigmoid output layer\n",
    "outputLayer = Dense(1, activation='softmax')(dot_product)\n",
    "\n",
    "model = Model(inputs=[input_word, input_context_word], outputs=outputLayer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# view model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVpBqKfo1iSj",
    "outputId": "64069770-8de7-4a1a-b8c9-a4657abe9f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 (skip_first, skip_second, relevance) pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\pyakc\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 2 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 3 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 4 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 5 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 6 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 7 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 8 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 9 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 10 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 11 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 12 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 13 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 14 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 15 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 16 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 17 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 18 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 19 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 20 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 21 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 22 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 23 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 24 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 25 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 26 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 27 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 28 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 29 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 30 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 31 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 32 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 33 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 34 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 35 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 36 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 37 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 38 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 39 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 40 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 41 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 42 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 43 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 44 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 45 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 46 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 47 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 48 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 49 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 50 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 51 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 52 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 53 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 54 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 55 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 56 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 57 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 58 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 59 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 60 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 61 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 62 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 63 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 64 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 65 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 66 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 67 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 68 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 69 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 70 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 71 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 72 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 73 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 74 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 75 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 76 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 77 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 78 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 79 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 80 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 81 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 82 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 83 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 84 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 85 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 86 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 87 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 88 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 89 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 90 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 91 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 92 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 93 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 94 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 95 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 96 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 97 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 98 Loss: 53.666337966918945\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 Loss: 53.666337966918945\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    loss = 0\n",
    "    for i, elem in enumerate(skip_grams):\n",
    "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [pair_first_elem, pair_second_elem]\n",
    "        Y = labels\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CjhkbLsp1k0Y"
   },
   "outputs": [],
   "source": [
    "#get the embeding matrix\n",
    "weights = model.get_weights()\n",
    "## Save the wordvectors\n",
    "f = open('skipgram_vectors.txt' ,'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for key in vocab:\n",
    "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
    "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4rUPCJC1mvc",
    "outputId": "9a4dcaa9-c273-4c5f-fbff-f3366d008770"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('that', 0.17893089354038239),\n",
       " ('relation', 0.1536899358034134),\n",
       " ('users', 0.14901486039161682),\n",
       " ('corpora', 0.14564503729343414),\n",
       " ('at', 0.1416715681552887),\n",
       " ('support', 0.13820742070674896),\n",
       " ('experimental', 0.13639777898788452),\n",
       " ('have', 0.1275327354669571),\n",
       " ('present', 0.11706452071666718),\n",
       " ('almost', 0.11317339539527893)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the vectors back and validate\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./skipgram_vectors.txt', binary=False)\n",
    "w2v.most_similar(positive=['language'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Vs8ng_Zf1o08"
   },
   "outputs": [],
   "source": [
    "#Excerise: \n",
    "#modeify the skipegram_model to share the same embeding layer between word and context\n",
    "#Discussion: which is better? Why?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvVPjiFn_iK7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZHLK3dN_8jW"
   },
   "source": [
    "Answer : On a top level ,it appears with bigger corpus and by including unknown words also making the model context dependent will increase accuracy \n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3PhBFeN_-qH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Word2Vec_workshop_Shivangi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
