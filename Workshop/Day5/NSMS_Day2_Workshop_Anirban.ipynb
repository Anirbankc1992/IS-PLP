{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine best N features from Uni+Bigram+Negation  \n",
    "together with w2v and glove feature  \n",
    "apply SVM/LogRegress(N feateures+w2v+glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anirban\\Desktop\\nus-iss\\Practical_Language_Processing\\Workshop\\Day6\n"
     ]
    }
   ],
   "source": [
    "##prepare dataset\n",
    "from os import getcwd, chdir\n",
    "fpath = getcwd()\n",
    "print(fpath)\n",
    "# Change your path here\n",
    "chdir(fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\anirban\\\\Desktop\\\\nus-iss\\\\Practical_Language_Processing\\\\Workshop\\\\Day6\\\\Data\\\\trainset.pk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8736/2225928609.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m###################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\\\Data\\\\trainset.pk\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtestset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\\\Data\\\\testset.pk\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\anirban\\\\Desktop\\\\nus-iss\\\\Practical_Language_Processing\\\\Workshop\\\\Day6\\\\Data\\\\trainset.pk'"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "#Data Preparation\n",
    "###################################################\n",
    "import pickle as pk\n",
    "trainset = pk.load(open(fpath+\"\\\\Data\\\\trainset.pk\", \"rb\"))\n",
    "testset = pk.load(open(fpath+\"\\\\Data\\\\testset.pk\", \"rb\"))\n",
    "\n",
    "X_train = [t[0] for t in trainset]\n",
    "X_test = [t[0] for t in testset]\n",
    "\n",
    "X = X_train + X_test\n",
    "\n",
    "Y_train = [t[1] for t in trainset]\n",
    "Y_test = [t[1] for t in testset]\n",
    "\n",
    "y = Y_train + Y_test\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Kbest feature from Ngrams+Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1)\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD Features - Negation\n",
    "import re\n",
    "\n",
    "\n",
    "def nega_tag(text):\n",
    "    transformed = re.sub(r\"\\b(?:never|nothing|nowhere|noone|none|not|haven't|hasn't|hasnt|hadn't|hadnt|can't|cant|couldn't|couldnt|shouldn't|shouldnt|won't|wont|wouldn't|wouldnt|don't|dont|doesn't|doesnt|didn't|didnt|isnt|isn't|aren't|arent|aint|ain't|hardly|seldom)\\b[\\w\\s]+[^\\w\\s]\", lambda match: re.sub(\n",
    "        r'(\\s+)(\\w+)', r'\\1NEG_\\2', match.group(0)), text, flags=re.IGNORECASE)\n",
    "    return(transformed)\n",
    "\n",
    "\n",
    "# Create a training list which will now contain reviews with Negatively tagged words and their labels\n",
    "train_set_nega = []\n",
    "\n",
    "# Append elements to the list\n",
    "for doc in trainset:\n",
    "    trans = nega_tag(doc[0])\n",
    "    lab = doc[1]\n",
    "    train_set_nega.append([trans, lab])\n",
    "\n",
    "print(train_set_nega[18])\n",
    "\n",
    "# Create a testing list which will now contain reviews with Negatively tagged words and their labels\n",
    "test_set_nega = []\n",
    "\n",
    "# Append elements to the list\n",
    "for doc in testset:\n",
    "    trans = nega_tag(doc[0])\n",
    "    lab = doc[1]\n",
    "    test_set_nega.append([trans, lab])\n",
    "\n",
    "\n",
    "print(test_set_nega[18])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redo - Preprocessing\n",
    "# seperate the text with labels\n",
    "\n",
    "X_nega_train = [t[0] for t in train_set_nega]\n",
    "X_nega_test = [t[0] for t in test_set_nega]\n",
    "\n",
    "Y_nega_train = [t[1] for t in train_set_nega]\n",
    "Y_nega_test = [t[1] for t in test_set_nega]\n",
    "\n",
    "#Vectorizer the sentences using Tfidf vale\n",
    "#Make sure test data should be transformed using vectorizer learned from training data\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1)\n",
    "train_nega_vectors = vectorizer.fit_transform(X_nega_train)\n",
    "test_nega_vectors = vectorizer.transform(X_nega_test)\n",
    "\n",
    "# bigger feature set\n",
    "print(train_vectors.shape)\n",
    "print(test_vectors.shape)\n",
    "\n",
    "print(train_nega_vectors.shape)\n",
    "print(test_nega_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch21 = SelectKBest(chi2, k=100)\n",
    "train_Kbest = ch21.fit_transform(train_nega_vectors, Y_nega_train)\n",
    "test_Kbest = ch21.transform(test_nega_vectors)\n",
    "train_Kbest.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "encoding = \"utf-8\"\n",
    "GLOVE_6B_100D_PATH = fpath+\"\\\\Data\\\\glove.6B.100d.txt\"\n",
    "with open(GLOVE_6B_100D_PATH, \"rb\") as lines:\n",
    "    wvec = {line.split()[0].decode(encoding): np.array(line.split()[1:], dtype=np.float32)\n",
    "            for line in lines}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading glove files, this may take a while\n",
    "# we're reading line by line and only saving vectors\n",
    "# that correspond to words from our data set\n",
    "\n",
    "glove_small = {}\n",
    "all_words = set(w for words in X for w in words)\n",
    "\n",
    "print(len(all_words))\n",
    "\n",
    "with open(GLOVE_6B_100D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums = np.array(parts[1:], dtype=np.float32)\n",
    "            glove_small[word] = nums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MeanEmbeddingVectorizer define the way to represent docs using word vectors\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec) > 0:\n",
    "            self.dim = len(word2vec[next(iter(glove_small))])\n",
    "            #self.dim=len(word2vec[\"and\"])\n",
    "        else:\n",
    "            self.dim = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "#Prepare word embeddings by training from dataset\n",
    "model = Word2Vec(X, vector_size=100, window=5, min_count=2, workers=2)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index_to_key, model.wv.vectors)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_Embedding = MeanEmbeddingVectorizer(w2v)\n",
    "glove_Embedding = MeanEmbeddingVectorizer(glove_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grab w2v feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_train = w2v_Embedding.transform(X_train)\n",
    "print(X_w2v_train.shape)\n",
    "X_w2v_test = w2v_Embedding.transform(X_test)\n",
    "print(X_w2v_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grab glove feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_glove_train = glove_Embedding.transform(X_train)\n",
    "print(X_glove_train.shape)\n",
    "X_glove_test = glove_Embedding.transform(X_test)\n",
    "print(X_glove_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_w2v_train.shape, X_glove_train.shape, train_Kbest.shape)\n",
    "print(type(X_w2v_train), type(X_glove_train), type(train_Kbest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine the feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = np.c_[X_w2v_train, X_glove_train, train_Kbest.toarray()]\n",
    "print(final_train.shape)\n",
    "final_test = np.c_[X_w2v_test, X_glove_test, test_Kbest.toarray()]\n",
    "print(final_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model_svm = LinearSVC(C=1.0)\n",
    "sv = model_svm.fit(final_train, Y_train)\n",
    "predSVM = sv.predict(final_test)\n",
    "pred = list(predSVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the features from of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(Y_test, pred))\n",
    "print(np.mean(predSVM == Y_test))\n",
    "print(metrics.classification_report(Y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with individual models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unigram, bigram, negation vectors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm_Kbest = LinearSVC(C=1.0)\n",
    "sv_Kbest = model_svm_Kbest.fit(train_Kbest, Y_train)\n",
    "predSVM_Kbest = sv_Kbest.predict(test_Kbest)\n",
    "pred_Kbest = list(predSVM_Kbest)\n",
    "print(metrics.confusion_matrix(Y_test, pred_Kbest))\n",
    "print(np.mean(predSVM_Kbest == Y_test))\n",
    "print(metrics.classification_report(Y_test, pred_Kbest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word 2 vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm_w2v = LinearSVC(C=1.0)\n",
    "sv_w2v = model_svm_w2v.fit(X_w2v_train, Y_train)\n",
    "predSVM_w2v = sv_w2v.predict(X_w2v_test)\n",
    "pred_w2v = list(predSVM_w2v)\n",
    "print(metrics.confusion_matrix(Y_test, pred_w2v))\n",
    "print(np.mean(predSVM_w2v == Y_test))\n",
    "print(metrics.classification_report(Y_test, pred_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm_glove = LinearSVC(C=1.0)\n",
    "sv_glove = model_svm_glove.fit(X_glove_train, Y_train)\n",
    "predSVM_glove = sv_glove.predict(X_glove_test)\n",
    "pred_glove = list(predSVM_glove)\n",
    "print(metrics.confusion_matrix(Y_test, pred_glove))\n",
    "print(np.mean(predSVM_glove == Y_test))\n",
    "print(metrics.classification_report(Y_test, pred_glove))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### benchmark all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "all_models_train_test = [\n",
    "    (\"sv_final\", sv, final_train, final_test),\n",
    "    (\"sv_Kbest\", sv_Kbest, train_Kbest.toarray(), test_Kbest.toarray()),\n",
    "    (\"sv_w2v\", sv_w2v, X_w2v_train, X_w2v_test),\n",
    "    (\"sv_glove\", sv_glove, X_glove_train, X_glove_test)\n",
    "]\n",
    "\n",
    "##how the ranking depends on the amount of training data\n",
    "\n",
    "def benchmark(model, X, y, n, X_test, y_test):\n",
    "    scores = []\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, train_size=n, test_size=len(y)-n)\n",
    "\n",
    "    for train, test in sss.split(X, y):\n",
    "        X_train = X[train]\n",
    "        y_train = y[train]\n",
    "        scores.append(accuracy_score(\n",
    "            model.fit(X_train, y_train).predict(X_test), y_test))\n",
    "        break\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "train_sizes = [1000, 5000, 10000, 15000, 17900]\n",
    "table = []\n",
    "for name, model, X_train, X_test in all_models_train_test:\n",
    "    for n in train_sizes:\n",
    "        table.append({'model': name,\n",
    "                      'accuracy': benchmark(model, np.array(X_train), np.array(Y_train), n, X_test, Y_test),\n",
    "                      'train_size': n})\n",
    "df = pd.DataFrame(table)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "fig = sns.pointplot(x='train_size', y='accuracy', hue='model',\n",
    "                    data=df[df.model.map(lambda x: x in [\"sv_final\", \"sv_Kbest\", \"sv_w2v\", \"sv_glove\"])])\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "fig.set(ylabel=\"accuracy\")\n",
    "fig.set(xlabel=\"labeled training examples\")\n",
    "fig.set(title=\"R8 benchmark\")\n",
    "fig.set(ylabel=\"accuracy\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4dfba2e2abceafa0bd3781a9c1a14434dac3a8eab3a5479503b649906f1b968"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
